# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Lqlck99CJ9mr73Qmqn2yG96z219iW4s
"""

import os, json, math, re
from collections import Counter
import numpy as np
import matplotlib.pyplot as plt

BASE_DIR = "/content/webnlg_kg_text2kg_1000"

PAIRS_PATH = os.path.join(BASE_DIR, "pairs_train_10016.json")
# ✅ B/C 결과: 문장별 semantic symbol 개수(=보내는 triple 수)를 여기서 읽음
ABSTR_SSC_PATH = os.path.join(BASE_DIR, "semantic_symbol_abstraction_ssc_train_10016.json")
# ✅ L(SSC bits/triple) 얻기 위한 KG SSC
KG_SSC_PATH = os.path.join(BASE_DIR, "kg_triples_ssc_train_10016.json")

assert os.path.exists(PAIRS_PATH), PAIRS_PATH
assert os.path.exists(ABSTR_SSC_PATH), ABSTR_SSC_PATH
assert os.path.exists(KG_SSC_PATH), KG_SSC_PATH

# ----------------------------
# Config
# ----------------------------
REP = 3  # repetition BCC (rep=3이면 3배 전송)
USE_UNICODE_BYTES_FOR_BIT7 = False

# ----------------------------
# Load data
# ----------------------------
with open(PAIRS_PATH, "r", encoding="utf-8") as f:
    pairs = json.load(f)

with open(ABSTR_SSC_PATH, "r", encoding="utf-8") as f:
    abstr = json.load(f)

with open(KG_SSC_PATH, "r", encoding="utf-8") as f:
    kg_ssc = json.load(f)

# idx 매핑 (int로 통일)
def to_int_idx(x):
    try:
        return int(x)
    except:
        return x

pairs_by_idx = {to_int_idx(ex.get("idx", i)): ex for i, ex in enumerate(pairs)}
abstr_by_idx = {to_int_idx(ex.get("idx")): ex for ex in abstr if "idx" in ex}

# SSC bit length L (triple당 비트 수)
L = len(kg_ssc[0]["bits"])
print("SSC bits per triple L =", L)
print("pairs:", len(pairs), "abstr:", len(abstr))

# ----------------------------
# Huffman coding utilities (character-level)
# ----------------------------
class HuffNode:
    __slots__ = ("ch","freq","left","right")
    def __init__(self, ch=None, freq=0, left=None, right=None):
        self.ch = ch
        self.freq = freq
        self.left = left
        self.right = right

def build_huffman_code_lengths(char_freq: dict):
    import heapq
    heap = []
    uid = 0
    for ch, fr in char_freq.items():
        heapq.heappush(heap, (fr, uid, HuffNode(ch=ch, freq=fr)))
        uid += 1

    if len(heap) == 1:
        fr, _, node = heap[0]
        return {node.ch: 1}

    while len(heap) > 1:
        fr1, _, n1 = heapq.heappop(heap)
        fr2, _, n2 = heapq.heappop(heap)
        merged = HuffNode(ch=None, freq=fr1+fr2, left=n1, right=n2)
        heapq.heappush(heap, (merged.freq, uid, merged))
        uid += 1

    root = heap[0][2]
    lengths = {}

    def dfs(node, depth):
        if node.ch is not None:
            lengths[node.ch] = max(depth, 1)
            return
        dfs(node.left, depth+1)
        dfs(node.right, depth+1)

    dfs(root, 0)
    return lengths

# 코퍼스 기반 Huffman codebook 생성
all_text = "".join([ex["text"] for ex in pairs])
freq = Counter(all_text)
huff_len = build_huffman_code_lengths(freq)

def huffman_bits(text: str) -> int:
    return int(sum(huff_len.get(ch, 8) for ch in text))  # unseen 대비 8bit fallback

def bit7_bits(text: str) -> int:
    if USE_UNICODE_BYTES_FOR_BIT7:
        return int(8 * len(text.encode("utf-8")))
    else:
        return int(7 * len(text))

def ours_bits_from_abstr(idx: int) -> int:
    item = abstr_by_idx.get(idx)
    if item is None:
        return 0
    # ✅ 우리가 보내는 건 semantic_symbols_ssc의 개수 * L
    n_tr = len(item.get("semantic_symbols_ssc", []))
    return int(n_tr * L)

# ----------------------------
# Compute per-sample bits
# ----------------------------
rows = []
for ex in pairs:
    idx = to_int_idx(ex.get("idx"))
    text = ex["text"]
    n_chars = len(text)

    b_bit7  = bit7_bits(text)
    b_huff  = huffman_bits(text)
    b_ours  = ours_bits_from_abstr(idx)

    rows.append({
        "idx": idx,
        "chars": n_chars,
        "bit7_bcc": b_bit7 * REP,
        "huff_bcc": b_huff * REP,
        "ours_bcc": b_ours * REP,  # ours도 동일 BCC 적용 (공정 비교)
    })

print("Computed rows:", len(rows))

# ----------------------------
# Figure 4: bits vs sentence length (char bins)
# ----------------------------
bins = [(0,100), (100,200), (200,300), (300,400), (400,10**9)]
bin_labels = ["0-100","100-200","200-300","300-400","400+"]

def avg_in_bin(key):
    out = []
    for (lo,hi) in bins:
        vals = [r[key] for r in rows if (r["chars"] >= lo and r["chars"] < hi)]
        out.append(float(np.mean(vals)) if len(vals) else np.nan)
    return out

y_huff = avg_in_bin("huff_bcc")
y_bit7 = avg_in_bin("bit7_bcc")
y_ours = avg_in_bin("ours_bcc")

plt.figure(figsize=(7,4.5))
x = np.arange(len(bin_labels))
plt.plot(x, y_huff, marker="o", label="Huffman + BCC")
plt.plot(x, y_bit7, marker="^", label="Bit7 + BCC")
plt.plot(x, y_ours, marker="*", label="Our proposed cognitive SC")
plt.xticks(x, bin_labels)
plt.xlabel("Sentence length (char)")
plt.ylabel("Number of bits (b)")
plt.grid(True, alpha=0.3)
plt.legend()
plt.title("Fig.4-style: bits vs sentence length (train=10016)")
plt.tight_layout()
plt.show()

# ----------------------------
# Figure 5: cumulative bits vs number of texts
# ----------------------------
# ✅ 10016이니까 논문 느낌으로 더 촘촘히도 가능
counts = [200, 400, 600, 800, 1000, 2000, 5000, 10016]
counts = [c for c in counts if c <= len(rows)]

def cumulative_kb(key, n):
    total_bits = sum(r[key] for r in rows[:n])  # 앞에서 n개
    return float(total_bits / 1000.0)  # kb (논문 축 관례)

y2_huff = [cumulative_kb("huff_bcc", n) for n in counts]
y2_bit7 = [cumulative_kb("bit7_bcc", n) for n in counts]
y2_ours = [cumulative_kb("ours_bcc", n) for n in counts]

plt.figure(figsize=(7,4.5))
plt.plot(counts, y2_huff, marker="o", label="Huffman + BCC")
plt.plot(counts, y2_bit7, marker="^", label="Bit7 + BCC")
plt.plot(counts, y2_ours, marker="*", label="Our proposed cognitive SC")
plt.xlabel("Number of texts")
plt.ylabel("Number of bits (kb)")
plt.grid(True, alpha=0.3)
plt.legend()
plt.title("Fig.5-style: cumulative bits vs number of texts (train=10016)")
plt.tight_layout()
plt.show()

# ----------------------------
# Quick print: 평균 절감률
# ----------------------------
avg_huff = float(np.mean([r["huff_bcc"] for r in rows]))
avg_bit7 = float(np.mean([r["bit7_bcc"] for r in rows]))
avg_ours = float(np.mean([r["ours_bcc"] for r in rows]))

print("\n=== Average transmitted bits per text ===")
print(f"Huffman+BCC: {avg_huff:.1f} bits")
print(f"Bit7+BCC   : {avg_bit7:.1f} bits")
print(f"Ours+BCC   : {avg_ours:.1f} bits")

print("\n=== Reduction vs baselines ===")
print(f"vs Huffman: {100*(1-avg_ours/avg_huff):.1f}%")
print(f"vs Bit7   : {100*(1-avg_ours/avg_bit7):.1f}%")

# ----------------------------
# Health check: ours가 0이 너무 많은지
# ----------------------------
zero_ours = sum(1 for r in rows if r["ours_bcc"] == 0)
print("\n=== Health check ===")
print(f"ours_bcc==0: {zero_ours}/{len(rows)} ({100*zero_ours/len(rows):.2f}%)")

!pip -q install -U sentence-transformers

import os, json, random, math, re
import numpy as np
import matplotlib.pyplot as plt
import torch
from collections import Counter

# -------------------------
# Config
# -------------------------
BASE_DIR = "/content/webnlg_kg_text2kg_1000"
PAIRS_PATH      = os.path.join(BASE_DIR, "pairs_train_10016.json")
ABSTR_SSC_PATH  = os.path.join(BASE_DIR, "semantic_symbol_abstraction_ssc_train_10016.json")
KG_SSC_PATH     = os.path.join(BASE_DIR, "kg_triples_ssc_train_10016.json")
T5_DIR          = os.path.join(BASE_DIR, "t5_kg2text_pt")

assert os.path.exists(PAIRS_PATH), PAIRS_PATH
assert os.path.exists(ABSTR_SSC_PATH), ABSTR_SSC_PATH
assert os.path.exists(KG_SSC_PATH), KG_SSC_PATH
assert os.path.exists(T5_DIR), T5_DIR

REP = 3
P_LIST = [0.0, 0.05, 0.10, 0.15, 0.20]
N_SAMPLES = 200               # 120~500 추천
FILTER_EMPTY_OURS = True      # ours가 empty인 샘플 제외(공정성/안정성 ↑)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", device)

# -------------------------
# Load data
# -------------------------
with open(PAIRS_PATH, "r", encoding="utf-8") as f:
    pairs = json.load(f)

with open(ABSTR_SSC_PATH, "r", encoding="utf-8") as f:
    abstr = json.load(f)

with open(KG_SSC_PATH, "r", encoding="utf-8") as f:
    kg_ssc = json.load(f)

def to_int_idx(x):
    try: return int(x)
    except: return x

pairs_by_idx = {to_int_idx(ex.get("idx", i)): ex for i, ex in enumerate(pairs)}
abstr_by_idx = {to_int_idx(ex.get("idx")): ex for ex in abstr if "idx" in ex}

L = len(kg_ssc[0]["bits"])
print("SSC bits per triple:", L)

# KG codebook (for correction)
KG_CODES = np.array([it["bits"] for it in kg_ssc], dtype=np.int8)
bits2triple = {}
for it in kg_ssc:
    tri = it.get("triple")
    if not tri:
        # fallback for older format
        tri = [it.get("h"), it.get("r"), it.get("t")]
    bits2triple[tuple(it["bits"])] = tri

# -------------------------
# T5 KG2Text
# (평가 셀과 동일: paper-style linearize)
# -------------------------
from transformers import T5TokenizerFast, T5ForConditionalGeneration
t5_tok = T5TokenizerFast.from_pretrained(T5_DIR)
t5 = T5ForConditionalGeneration.from_pretrained(T5_DIR).to(device)
t5.eval()

MAX_SRC_LEN = 256

def clean_obj(x: str) -> str:
    x = str(x).replace("_", " ").replace('"', "").strip()
    if "^^<" in x:
        x = x.split("^^<")[0].strip()
    x = re.sub(r"\s+"," ", x)
    return x

def linearize_paper_style(triples):
    trs=[]
    for tri in triples:
        if isinstance(tri, dict) and "triple" in tri:
            tri = tri["triple"]
        if not isinstance(tri, (list, tuple)) or len(tri) < 3:
            continue
        h,r,t = tri[0], tri[1], tri[2]
        trs.append((clean_obj(h), clean_obj(r), clean_obj(t)))

    if not trs:
        return ""
    head_counts = Counter([h for h,_,_ in trs])
    main_head = head_counts.most_common(1)[0][0]
    main_pairs = [(r,t) for (h,r,t) in trs if h == main_head]
    if not main_pairs:
        main_head = trs[0][0]
        main_pairs = [(trs[0][1], trs[0][2])]
    rt = ", ".join([f"{r} {t}" for (r,t) in main_pairs])
    return f"kg2text: {main_head} {rt}"

@torch.no_grad()
def kg2text_generate(triples, max_new_tokens=80):
    src = linearize_paper_style(triples)
    if not src.strip():
        return ""
    inputs = t5_tok(src, return_tensors="pt", truncation=True, max_length=MAX_SRC_LEN).to(device)
    y = t5.generate(**inputs, num_beams=1, do_sample=False, max_new_tokens=max_new_tokens)
    return t5_tok.decode(y[0], skip_special_tokens=True)

# -------------------------
# Channel: BSC + repetition
# -------------------------
def bsc_flip_bits(bits: np.ndarray, p: float) -> np.ndarray:
    flip = (np.random.rand(bits.size) < p).astype(np.int8)
    return (bits ^ flip).astype(np.int8)

def rep_encode(bits: np.ndarray, rep=3):
    return np.repeat(bits, rep).astype(np.int8)

def rep_decode(bits: np.ndarray, rep=3):
    n = (bits.size // rep) * rep
    bits = bits[:n].reshape(-1, rep)
    return (np.sum(bits, axis=1) >= (rep/2)).astype(np.int8)

# correction: nearest neighbor in KG codebook (Hamming min)
def correction_algo(o: np.ndarray) -> np.ndarray:
    dists = np.sum(KG_CODES != o[None, :], axis=1)
    return KG_CODES[int(np.argmin(dists))]

# -------------------------
# Baselines: Bit7 + Huffman
# -------------------------
def bit7_encode(text: str) -> np.ndarray:
    arr=[]
    for ch in text:
        v = ord(ch) & 0x7F
        arr.extend([(v >> k) & 1 for k in range(6,-1,-1)])
    return np.array(arr, dtype=np.int8)

def bit7_decode(bits: np.ndarray) -> str:
    n = (bits.size // 7) * 7
    bits = bits[:n].reshape(-1, 7)
    chars=[]
    for b in bits:
        v=0
        for i in range(7):
            v = (v << 1) | int(b[i])
        chars.append(chr(v))
    return "".join(chars)

def build_huffman_codebook(text_corpus: str):
    import heapq
    freq = Counter(text_corpus)
    heap=[]
    uid=0
    for ch,fr in freq.items():
        heapq.heappush(heap,(fr,uid,(ch,None,None)))
        uid+=1
    if len(heap)==1:
        ch = heap[0][2][0]
        return {ch:"0"}
    while len(heap)>1:
        fr1,_,n1=heapq.heappop(heap)
        fr2,_,n2=heapq.heappop(heap)
        heapq.heappush(heap,(fr1+fr2,uid,(None,n1,n2))); uid+=1
    root=heap[0][2]
    codes={}
    def dfs(node,prefix):
        ch,left,right = node
        if ch is not None:
            codes[ch]=prefix or "0"
            return
        dfs(left,prefix+"0")
        dfs(right,prefix+"1")
    dfs(root,"")
    return codes

all_text = "".join([ex["text"] for ex in pairs])
huff_code = build_huffman_codebook(all_text)

class HuffTrie:
    def __init__(self): self.root={}
    def add(self,ch,code):
        cur=self.root
        for c in code:
            cur=cur.setdefault(c,{})
        cur["$"]=ch
    def decode(self,bits):
        cur=self.root
        out=[]
        for b in bits:
            cur=cur.get("1" if b else "0", {})
            if "$" in cur:
                out.append(cur["$"])
                cur=self.root
        return "".join(out)

trie = HuffTrie()
for ch,code in huff_code.items():
    trie.add(ch,code)

def huff_encode(text: str) -> np.ndarray:
    bits=[]
    for ch in text:
        code = huff_code.get(ch)
        if code is None:
            code = format(ord(ch)&0xFF, "08b")
        bits.extend([1 if c=="1" else 0 for c in code])
    return np.array(bits, dtype=np.int8)

def huff_decode(bits: np.ndarray) -> str:
    return trie.decode(bits.tolist())

def bit7_baseline_reconstruct(text: str, p: float, rep=3):
    bits = bit7_encode(text)
    tx = rep_encode(bits, rep=rep)
    rx = bsc_flip_bits(tx, p)
    dec = rep_decode(rx, rep=rep)
    return bit7_decode(dec)

def huff_baseline_reconstruct(text: str, p: float, rep=3):
    bits = huff_encode(text)
    tx = rep_encode(bits, rep=rep)
    rx = bsc_flip_bits(tx, p)
    dec = rep_decode(rx, rep=rep)
    return huff_decode(dec)

# -------------------------
# OURS: (semantic_symbols_ssc bits) -> channel -> correction -> triples -> T5
# -------------------------
def ours_reconstruct(idx: int, p: float, rep=3):
    item = abstr_by_idx.get(idx)
    if item is None:
        return ""

    syms = item.get("semantic_symbols_ssc", [])
    if not syms:
        return ""

    final_triples = []
    for s in syms:
        b = s.get("bits")
        if b is None:
            continue
        code = np.array(b, dtype=np.int8)

        tx = rep_encode(code, rep=rep)
        rx = bsc_flip_bits(tx, p)
        dec = rep_decode(rx, rep=rep)
        corr = correction_algo(dec)

        tri_rec = bits2triple.get(tuple(corr.tolist()))
        if tri_rec is not None and len(tri_rec) >= 3:
            final_triples.append([tri_rec[0], tri_rec[1], tri_rec[2]])

    if not final_triples:
        return ""

    return kg2text_generate(final_triples)

# -------------------------
# Sentence similarity embeddings (cosine)
# -------------------------
from sentence_transformers import SentenceTransformer
emb_model = SentenceTransformer("all-MiniLM-L6-v2", device=device)

def embed_sentences(texts):
    return emb_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)

def cosine_sim(a, b):
    return np.sum(a*b, axis=1)

# -------------------------
# BLEU-1..4 (stable)
# -------------------------
def ngrams(tokens, n):
    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

def corpus_bleu_n(preds, refs, n):
    total_clipped = 0
    total_count = 0
    pred_len = 0
    ref_len = 0

    for ptxt, rtxt in zip(preds, refs):
        ptok = ptxt.strip().split()
        rtok = rtxt.strip().split()
        pred_len += len(ptok)
        ref_len  += len(rtok)

        if len(ptok) < n:
            continue

        p_ng = Counter(ngrams(ptok, n))
        r_ng = Counter(ngrams(rtok, n))
        clipped = sum((p_ng & r_ng).values())
        count = sum(p_ng.values())

        total_clipped += clipped
        total_count += count

    prec = (total_clipped / total_count) if total_count > 0 else 0.0

    if pred_len == 0:
        bp = 0.0
    elif pred_len > ref_len:
        bp = 1.0
    else:
        bp = math.exp(1 - (ref_len / max(pred_len, 1)))

    return 100.0 * bp * prec

# -------------------------
# Sample selection (공정하게: ours가 비어있는 샘플 제외 옵션)
# -------------------------
random.seed(7)
all_idxs = [to_int_idx(ex.get("idx", i)) for i, ex in enumerate(pairs)]

if FILTER_EMPTY_OURS:
    valid_idxs = [idx for idx in all_idxs if abstr_by_idx.get(idx, {}).get("semantic_symbols_ssc")]
    print("valid (non-empty ours):", len(valid_idxs), "/", len(all_idxs))
else:
    valid_idxs = all_idxs

eval_idxs = random.sample(valid_idxs, min(N_SAMPLES, len(valid_idxs)))
eval_samples = [pairs_by_idx[idx] for idx in eval_idxs]

# -------------------------
# Run sweep
# -------------------------
sim_curve = {"huff":[], "bit7":[], "ours":[]}
bleu_curve = {"huff":[[],[],[],[]], "bit7":[[],[],[],[]], "ours":[[],[],[],[]]}
empty_rates = []

def run_one_p(p):
    gold_texts, rec_huff, rec_bit7, rec_ours = [], [], [], []

    for ex in eval_samples:
        idx = to_int_idx(ex.get("idx"))
        gold = ex["text"]
        gold_texts.append(gold)

        rec_huff.append(huff_baseline_reconstruct(gold, p, rep=REP))
        rec_bit7.append(bit7_baseline_reconstruct(gold, p, rep=REP))
        rec_ours.append(ours_reconstruct(idx, p, rep=REP))

    # similarity
    eg = embed_sentences(gold_texts)
    eh = embed_sentences(rec_huff)
    eb = embed_sentences(rec_bit7)
    eo = embed_sentences(rec_ours)

    sim_h = float(np.mean(cosine_sim(eg, eh)))
    sim_b = float(np.mean(cosine_sim(eg, eb)))
    sim_o = float(np.mean(cosine_sim(eg, eo)))

    # BLEU-1..4
    b_h = [corpus_bleu_n(rec_huff, gold_texts, n) for n in [1,2,3,4]]
    b_b = [corpus_bleu_n(rec_bit7, gold_texts, n) for n in [1,2,3,4]]
    b_o = [corpus_bleu_n(rec_ours, gold_texts, n) for n in [1,2,3,4]]

    empty_rate = sum([1 for s in rec_ours if s.strip()==""]) / len(rec_ours)
    return (sim_h, sim_b, sim_o), b_h, b_b, b_o, empty_rate

for p in P_LIST:
    (s_h, s_b, s_o), bh, bb, bo, er = run_one_p(p)
    sim_curve["huff"].append(s_h)
    sim_curve["bit7"].append(s_b)
    sim_curve["ours"].append(s_o)
    for k in range(4):
        bleu_curve["huff"][k].append(bh[k])
        bleu_curve["bit7"][k].append(bb[k])
        bleu_curve["ours"][k].append(bo[k])
    empty_rates.append(er)
    print(f"p={p:.2f} | ours empty rate={er:.2%} | sim(ours)={s_o:.4f}")

print("\nDone sweep.")

# -------------------------
# Plot Fig.6
# -------------------------
plt.figure(figsize=(7,4.5))
plt.plot(P_LIST, sim_curve["huff"], marker="o", label="Huffman + BCC")
plt.plot(P_LIST, sim_curve["bit7"], marker="^", label="Bit7 + BCC")
plt.plot(P_LIST, sim_curve["ours"], marker="*", label="Our proposed cognitive SC")
plt.xlabel("BSC wrong parameter p")
plt.ylabel("Semantic similarity score (cosine)")
plt.grid(True, alpha=0.3)
plt.legend()
plt.title("Fig.6-style: Semantic similarity vs p (train=10016)")
plt.tight_layout()
plt.show()

# -------------------------
# Plot Fig.7 (BLEU-1..4 in 2x2)
# -------------------------
titles = ["BLEU-1", "BLEU-2", "BLEU-3", "BLEU-4"]
plt.figure(figsize=(10,7.5))
for i in range(4):
    ax = plt.subplot(2,2,i+1)
    ax.plot(P_LIST, bleu_curve["huff"][i], marker="o", label="Huffman + BCC")
    ax.plot(P_LIST, bleu_curve["bit7"][i], marker="^", label="Bit7 + BCC")
    ax.plot(P_LIST, bleu_curve["ours"][i], marker="*", label="Our proposed cognitive SC")
    ax.set_title(titles[i])
    ax.set_xlabel("BSC wrong parameter p")
    ax.set_ylabel("BLEU score")
    ax.grid(True, alpha=0.3)
    if i == 1:
        ax.legend(loc="best")
plt.tight_layout()
plt.show()