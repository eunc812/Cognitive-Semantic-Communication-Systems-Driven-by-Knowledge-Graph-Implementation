# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Lqlck99CJ9mr73Qmqn2yG96z219iW4s
"""



import os, json, random, re, math
from collections import Counter
from typing import List, Dict

import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from transformers import T5TokenizerFast, T5ForConditionalGeneration

# -----------------------
# Config
# -----------------------
BASE_DIR = "/content/webnlg_kg_text2kg_1000"
PAIR_PATH = os.path.join(BASE_DIR, "pairs_train_10016.json")
assert os.path.exists(PAIR_PATH), f"Missing {PAIR_PATH}"

MODEL_NAME = "t5-small"
OUT_DIR = os.path.join(BASE_DIR, "t5_kg2text_pt")
os.makedirs(OUT_DIR, exist_ok=True)

SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", device)

MAX_SRC_LEN = 256
MAX_TGT_LEN = 128
BATCH_SIZE  = 8
EPOCHS      = 2        # 먼저 1~2 epoch 추천 (원하면 5~10으로)
LR          = 3e-4
GRAD_CLIP   = 1.0

# -----------------------
# Load pairs (train=10016)
# -----------------------
with open(PAIR_PATH, "r", encoding="utf-8") as f:
    pairs = json.load(f)

print("Loaded pairs:", len(pairs))
print("Example text:", pairs[0]["text"][:120])
print("Example triples:", pairs[0]["triples"][:3])

random.shuffle(pairs)
n_train = int(len(pairs) * 0.95)
train_pairs = pairs[:n_train]
val_pairs   = pairs[n_train:]
print("train:", len(train_pairs), "val:", len(val_pairs))

# -----------------------
# Linearize triples (논문 입력 스타일)
# "head relation1 tail1, relation2 tail2, ..."
# -----------------------
def clean_obj(x: str) -> str:
    x = str(x).replace("_", " ").replace('"', "").strip()
    if "^^<" in x:
        x = x.split("^^<")[0].strip()
    x = re.sub(r"\s+", " ", x)
    return x

def linearize_paper_style(triples: List[List[str]]) -> str:
    trs = []
    for tri in triples:
        if not isinstance(tri, (list, tuple)) or len(tri) < 3:
            continue
        h, r, t = tri[0], tri[1], tri[2]
        trs.append((clean_obj(h), clean_obj(r), clean_obj(t)))

    if not trs:
        return ""

    # main head: 가장 많이 나오는 head
    head_counts = Counter([h for h,_,_ in trs])
    main_head = head_counts.most_common(1)[0][0]

    main_pairs = [(r,t) for (h,r,t) in trs if h == main_head]
    if not main_pairs:
        main_head = trs[0][0]
        main_pairs = [(trs[0][1], trs[0][2])]

    rt = ", ".join([f"{r} {t}" for (r,t) in main_pairs])
    return f"kg2text: {main_head} {rt}"

# -----------------------
# Dataset / Collate
# -----------------------
tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)

class KG2TextDataset(Dataset):
    def __init__(self, items):
        self.items = items
    def __len__(self):
        return len(self.items)
    def __getitem__(self, i):
        ex = self.items[i]
        src = linearize_paper_style(ex["triples"])
        tgt = ex["text"]
        return {"src": src, "tgt": tgt}

def collate_fn(batch: List[Dict]):
    src_texts = [b["src"] for b in batch]
    tgt_texts = [b["tgt"] for b in batch]

    enc = tokenizer(
        src_texts,
        max_length=MAX_SRC_LEN,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )

    lab = tokenizer(
        text_target=tgt_texts,
        max_length=MAX_TGT_LEN,
        truncation=True,
        padding=True,
        return_tensors="pt"
    )
    labels = lab["input_ids"]
    labels[labels == tokenizer.pad_token_id] = -100
    enc["labels"] = labels
    return enc

train_loader = DataLoader(KG2TextDataset(train_pairs), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
val_loader   = DataLoader(KG2TextDataset(val_pairs),   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)

# -----------------------
# Model / Optim
# -----------------------
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=LR)

# -----------------------
# Eval
# -----------------------
@torch.no_grad()
def eval_loss():
    model.eval()
    tot, cnt = 0.0, 0
    for batch in val_loader:
        batch = {k:v.to(device) for k,v in batch.items()}
        out = model(**batch)
        bs = batch["input_ids"].size(0)
        tot += out.loss.item() * bs
        cnt += bs
    model.train()
    return tot / max(cnt, 1)

# -----------------------
# Train
# -----------------------
print("\nStarting fine-tuning...")
for epoch in range(1, EPOCHS+1):
    model.train()
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}")
    running, seen = 0.0, 0

    for batch in pbar:
        batch = {k:v.to(device) for k,v in batch.items()}
        out = model(**batch)
        loss = out.loss

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
        optimizer.step()

        bs = batch["input_ids"].size(0)
        running += loss.item() * bs
        seen += bs
        pbar.set_postfix(train_loss=running/max(seen,1))

    v = eval_loss()
    print(f"Epoch {epoch} done | train_loss={running/max(seen,1):.4f} | val_loss={v:.4f}")

# -----------------------
# Save
# -----------------------
model.save_pretrained(OUT_DIR)
tokenizer.save_pretrained(OUT_DIR)
print("Saved fine-tuned model to:", OUT_DIR)

# -----------------------
# Quick demo
# -----------------------
@torch.no_grad()
def gen_one(triples, max_new_tokens=80):
    src = linearize_paper_style(triples)
    inp = tokenizer(src, return_tensors="pt", truncation=True, max_length=MAX_SRC_LEN).to(device)
    y = model.generate(**inp, num_beams=4, do_sample=False, max_new_tokens=max_new_tokens)
    return src, tokenizer.decode(y[0], skip_special_tokens=True)

demo = val_pairs[0]
src, out = gen_one(demo["triples"])
print("\n====== DEMO (Fine-tuned) ======")
print("INPUT:", src[:220])
print("GOLD :", demo["text"][:220])
print("GEN  :", out[:220])
