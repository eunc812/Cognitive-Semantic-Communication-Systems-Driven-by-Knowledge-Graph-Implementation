# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Lqlck99CJ9mr73Qmqn2yG96z219iW4s
"""

import os, json, math, re
from collections import defaultdict
from tqdm import tqdm

BASE_DIR = "/content/webnlg_kg_text2kg_1000"

# ✅ 너가 지금 만든 10k(train=10016) 파일들
KG_SSC_PATH = os.path.join(BASE_DIR, "kg_triples_ssc_train_10016.json")
B_OUT_PATH  = os.path.join(BASE_DIR, "semantic_symbol_abstraction_train_10016.json")

assert os.path.exists(KG_SSC_PATH), f"Missing: {KG_SSC_PATH}"
assert os.path.exists(B_OUT_PATH),  f"Missing: {B_OUT_PATH}"

with open(KG_SSC_PATH, "r", encoding="utf-8") as f:
    kg_ssc = json.load(f)  # each: {"triple":[h,r,t], "bits":[...]}
with open(B_OUT_PATH, "r", encoding="utf-8") as f:
    abstr = json.load(f)   # each: {"idx","text","semantic_symbols":[{"triple":[...],"bits":[...]}]}

print("KG:", KG_SSC_PATH)
print("B out:", B_OUT_PATH)
print("KG triples:", len(kg_ssc))
print("B items:", len(abstr))

# -------------------------
# normalize (dictionary key 안정화용)
# -------------------------
def strip_datatype(x: str) -> str:
    x = str(x).strip()
    if "^^<" in x:
        x = x.split("^^<")[0].strip()
    return x

def norm_key(x: str) -> str:
    x = strip_datatype(x)
    x = x.replace("_"," ").replace('"'," ").strip()
    x = re.sub(r"^<http[^>]+/([^/>]+)>$", r"\1", x)
    x = re.sub(r"\s+"," ", x)
    return x.lower()

def norm_triple_key(tri):
    h,r,t = tri
    return (norm_key(h), norm_key(r), norm_key(t))

# -------------------------
# 1) Build dictionaries: entity2id, rel2id
# -------------------------
entities=set()
relations=set()

for it in kg_ssc:
    tri = it.get("triple", None)
    if not tri or len(tri) < 3:
        continue
    h,r,t = tri[0], tri[1], tri[2]
    entities.add(strip_datatype(h))
    entities.add(strip_datatype(t))
    relations.add(str(r))

entities = sorted(list(entities))
relations = sorted(list(relations))

entity2id = {e:i for i,e in enumerate(entities)}
rel2id    = {r:i for i,r in enumerate(relations)}

print("Num entities:", len(entity2id))
print("Num relations:", len(rel2id))

# bit-length (fixed)
bE = math.ceil(math.log2(len(entity2id))) if len(entity2id) > 1 else 1
bR = math.ceil(math.log2(len(rel2id))) if len(rel2id) > 1 else 1
print("Bits per entity:", bE, "| Bits per relation:", bR, "| Total bits per triple:", 2*bE + bR)

# save dicts
with open(os.path.join(BASE_DIR, "entity2id_train_10016.json"), "w", encoding="utf-8") as f:
    json.dump(entity2id, f, ensure_ascii=False, indent=2)
with open(os.path.join(BASE_DIR, "rel2id_train_10016.json"), "w", encoding="utf-8") as f:
    json.dump(rel2id, f, ensure_ascii=False, indent=2)

print("Saved dictionaries:",
      os.path.join(BASE_DIR, "entity2id_train_10016.json"),
      os.path.join(BASE_DIR, "rel2id_train_10016.json"))

# -------------------------
# 2) Attach integer ids to KG SSC
#    (bits는 기존 파일 것을 그대로 사용)
# -------------------------
def triple_to_ids(tri):
    h,r,t = tri
    h = strip_datatype(h); t = strip_datatype(t)
    r = str(r)
    return int(entity2id[h]), int(rel2id[r]), int(entity2id[t])

kg_ssc_with_ids = []
# lookup: norm triple -> (bits, raw triple)
kg_lookup = {}
for it in kg_ssc:
    tri = it["triple"]
    key = norm_triple_key(tri)
    kg_lookup[key] = {"bits": it["bits"], "triple": tri}

for it in kg_ssc:
    tri = it["triple"]
    hid, rid, tid = triple_to_ids(tri)
    kg_ssc_with_ids.append({
        "triple": tri,
        "hid": hid, "rid": rid, "tid": tid,
        "bits": it["bits"]
    })

OUT_KG_SSC_IDS = os.path.join(BASE_DIR, "kg_triples_ssc_with_ids_train_10016.json")
with open(OUT_KG_SSC_IDS, "w", encoding="utf-8") as f:
    json.dump(kg_ssc_with_ids, f, ensure_ascii=False, indent=2)

print("Saved:", OUT_KG_SSC_IDS)

# -------------------------
# 3) Attach ids (and fill missing bits) to B output
# -------------------------
fixed = 0
missing = 0

for ex in abstr:
    new_syms=[]
    for sym in ex.get("semantic_symbols", []):
        tri = sym.get("triple", None)
        if not tri or len(tri) < 3:
            continue

        # bits가 없으면 KG에서 채워줌
        bits = sym.get("bits", None)
        if bits is None:
            key = norm_triple_key(tri)
            if key in kg_lookup:
                bits = kg_lookup[key]["bits"]
                fixed += 1
            else:
                missing += 1
                continue

        hid, rid, tid = triple_to_ids(tri)
        new_syms.append({
            "triple": tri,
            "hid": hid, "rid": rid, "tid": tid,
            "bits": bits
        })
    ex["semantic_symbols_ssc"] = new_syms

OUT_ABSTR_SSC = os.path.join(BASE_DIR, "semantic_symbol_abstraction_ssc_train_10016.json")
with open(OUT_ABSTR_SSC, "w", encoding="utf-8") as f:
    json.dump(abstr, f, ensure_ascii=False, indent=2)

print("Saved:", OUT_ABSTR_SSC)
print("Filled missing bits from KG:", fixed)
print("Missing triples not found in KG:", missing)

# sanity
for ex in abstr:
    if ex.get("semantic_symbols_ssc"):
        print("\n--- SANITY ---")
        print("idx:", ex.get("idx"))
        print("text:", ex.get("text","")[:120])
        print("first symbol:", ex["semantic_symbols_ssc"][0])
        break
import os, json, numpy as np
from tqdm import tqdm

BASE_DIR = "/content/webnlg_kg_text2kg_1000"

ABSTR_SSC_PATH   = os.path.join(BASE_DIR, "semantic_symbol_abstraction_ssc_train_10016.json")
KG_SSC_IDS_PATH  = os.path.join(BASE_DIR, "kg_triples_ssc_with_ids_train_10016.json")

assert os.path.exists(ABSTR_SSC_PATH), ABSTR_SSC_PATH
assert os.path.exists(KG_SSC_IDS_PATH), KG_SSC_IDS_PATH

with open(ABSTR_SSC_PATH, "r", encoding="utf-8") as f:
    abstr = json.load(f)   # each: {"idx","text",...,"semantic_symbols_ssc":[{triple,hid,rid,tid,bits}]}
with open(KG_SSC_IDS_PATH, "r", encoding="utf-8") as f:
    kg = json.load(f)      # each: {"triple":[h,r,t], "hid","rid","tid","bits"}

L = len(kg[0]["bits"])
print("Loaded abstr:", len(abstr))
print("Loaded KG:", len(kg))
print("SSC bits length L:", L)

# -------------------------
# D parameters
# -------------------------
P = 0.10         # BSC error prob (원하면 0.0~0.2로 바꿔)
REP = 3          # repetition code
OUT_PATH = os.path.join(BASE_DIR, f"channel_decoded_symbols_train_10016_p{int(P*100):02d}_rep{REP}.json")

# -------------------------
# Channel coding / channel / decoding
# -------------------------
def rep_encode(bits: np.ndarray, rep=3) -> np.ndarray:
    return np.repeat(bits, rep).astype(np.int8)

def rep_decode(bits: np.ndarray, rep=3) -> np.ndarray:
    n = (bits.size // rep) * rep
    bits = bits[:n].reshape(-1, rep)
    return (np.sum(bits, axis=1) >= (rep/2)).astype(np.int8)

def bsc(bits: np.ndarray, p: float) -> np.ndarray:
    flip = (np.random.rand(bits.size) < p).astype(np.int8)
    return (bits ^ flip).astype(np.int8)

# -------------------------
# Build KG codebook arrays for correction
# -------------------------
KG_BITS = np.array([it["bits"] for it in kg], dtype=np.int8)

# bits -> triple/ids 역매핑 (bits는 list라 tuple로)
bits2meta = {}
for it in kg:
    bits2meta[tuple(it["bits"])] = {
        "triple": it["triple"],
        "hid": it["hid"], "rid": it["rid"], "tid": it["tid"],
        "bits": it["bits"]
    }

def correction(o_bits: np.ndarray) -> dict:
    """
    Table II의 'most similar semantic symbol'을 구현.
    - 논문 문맥상: Hamming distance 최소가 가장 유사.
    """
    # brute-force Hamming to all KG codes
    dists = np.sum(KG_BITS != o_bits[None, :], axis=1)
    best = int(np.argmin(dists))
    best_bits = tuple(KG_BITS[best].tolist())
    return bits2meta[best_bits]

# -------------------------
# Run over dataset (D)
# -------------------------
out = []
empty_in = 0
empty_out = 0

for ex in tqdm(abstr, total=len(abstr)):
    syms = ex.get("semantic_symbols_ssc", [])
    if not syms:
        empty_in += 1
        out.append({
            "idx": ex.get("idx"),
            "text": ex.get("text"),
            "decoded_symbols": []
        })
        continue

    decoded = []
    for s in syms:
        b = np.array(s["bits"], dtype=np.int8)
        tx = rep_encode(b, rep=REP)
        rx = bsc(tx, P)
        o  = rep_decode(rx, rep=REP)

        meta = correction(o)  # corrected symbol from KG
        decoded.append(meta)

    if not decoded:
        empty_out += 1

    out.append({
        "idx": ex.get("idx"),
        "text": ex.get("text"),
        "decoded_symbols": decoded
    })

with open(OUT_PATH, "w", encoding="utf-8") as f:
    json.dump(out, f, ensure_ascii=False, indent=2)

print("\nSaved:", OUT_PATH)
print("Empty input (no symbols):", empty_in, "/", len(abstr))
print("Empty output (after decode):", empty_out, "/", len(abstr))

# -------------------------
# Demo print (1 example)
# -------------------------
for ex in out:
    if ex["decoded_symbols"]:
        print("\n==== DEMO (D output) ====")
        print("idx:", ex["idx"])
        print("text:", (ex["text"] or "")[:200])
        print("decoded_symbols count:", len(ex["decoded_symbols"]))
        print("first decoded triple:", ex["decoded_symbols"][0]["triple"])
        print("first decoded ids:", (ex["decoded_symbols"][0]["hid"], ex["decoded_symbols"][0]["rid"], ex["decoded_symbols"][0]["tid"]))
        break