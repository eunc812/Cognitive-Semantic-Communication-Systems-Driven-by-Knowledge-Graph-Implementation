# -*- coding: utf-8 -*-
"""preprocess.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QeJ3OpW_C962qPP5zfgTsOLGn0qNtJ2Z
"""


import os, json, random, re, math
from collections import Counter, defaultdict
import numpy as np

BASE_DIR = "/content/webnlg_kg_text2kg_1000"
os.makedirs(BASE_DIR, exist_ok=True)

OUT_PAIRS   = os.path.join(BASE_DIR, "pairs_train_10016.json")
OUT_ALIGNED = os.path.join(BASE_DIR, "text2kg_aligned_train_10016.json")
OUT_SSC     = os.path.join(BASE_DIR, "kg_triples_ssc_train_10016.json")

SEED = 42
random.seed(SEED)

# ---------- load WebNLG (Orange/webnlg-qa가 너 환경에서 이미 동작했었음) ----------
from datasets import load_dataset
ds = load_dataset("Orange/webnlg-qa", split="train")   # rows=10016
print("rows:", len(ds))
print("keys:", ds.column_names)

def pick_text(row):
    # row["lex"]["text"]는 list 형태
    lex = row.get("lex", {})
    texts = lex.get("text", []) if isinstance(lex, dict) else []
    if texts and isinstance(texts, list):
        return texts[0]
    return ""

def pick_triples(row):
    # row["modified_triple_sets"]가 list[dict{subject,property,object}]
    triples = row.get("modified_triple_sets", [])
    out=[]
    for tri in triples:
        if isinstance(tri, dict):
            h = tri.get("subject"); r = tri.get("property"); t = tri.get("object")
            if h is not None and r is not None and t is not None:
                out.append([h, r, t])
        elif isinstance(tri, (list, tuple)) and len(tri) >= 3:
            out.append([tri[0], tri[1], tri[2]])
    return out

pairs=[]
for i in range(len(ds)):
    row = ds[i]
    text = pick_text(row)
    triples = pick_triples(row)
    if text.strip() and len(triples)>0:
        pairs.append({"idx": int(row.get("id", i)), "text": text, "triples": triples})

print("pairs:", len(pairs))
with open(OUT_PAIRS, "w", encoding="utf-8") as f:
    json.dump(pairs, f, ensure_ascii=False, indent=2)
print("saved:", OUT_PAIRS)

# ---------- Text2KG alignment (Table I 스타일) ----------
def _strip_uri_datatype(x: str) -> str:
    x = str(x).strip()
    if "^^<" in x:
        x = x.split("^^<")[0].strip()
    return x

def norm_entity(x: str) -> str:
    x = _strip_uri_datatype(x)
    x = x.replace("_"," ").replace('"',"").strip()
    x = re.sub(r"^<http[^>]+/([^/>]+)>$", r"\1", x)
    x = re.sub(r"\s+"," ", x)
    return x.lower()

def align_triples_by_contains(text: str, triples):
    s = text.lower()
    out=[]
    for h,r,t in triples:
        hh = norm_entity(h); tt = norm_entity(t)
        # 논문 Table I 조건: sentence contains(h) and contains(t)
        if hh and tt and (hh in s) and (tt in s):
            out.append([h,r,t])
    return out

aligned=[]
for ex in pairs:
    at = align_triples_by_contains(ex["text"], ex["triples"])
    aligned.append({"idx": ex["idx"], "aligned_triples": at, "gold_triples": ex["triples"]})

with open(OUT_ALIGNED, "w", encoding="utf-8") as f:
    json.dump(aligned, f, ensure_ascii=False, indent=2)
print("saved:", OUT_ALIGNED)

# ---------- SSC (dictionary + fixed-length binary) ----------
# 1) vocab 만들기
ents=set()
rels=set()
all_triples=[]
for ex in pairs:
    for h,r,t in ex["triples"]:
        ents.add(_strip_uri_datatype(h))
        ents.add(_strip_uri_datatype(t))
        rels.add(str(r))
        all_triples.append([h,r,t])

ent2id = {e:i for i,e in enumerate(sorted(ents))}
rel2id = {r:i for i,r in enumerate(sorted(rels))}
# triple code length bits
B_ent = int(math.ceil(math.log2(len(ent2id)+1)))
B_rel = int(math.ceil(math.log2(len(rel2id)+1)))
L = B_ent + B_rel + B_ent
print("vocab ents:", len(ent2id), "rels:", len(rel2id))
print("bits:", "B_ent", B_ent, "B_rel", B_rel, "L", L)

def int_to_bits(x, width):
    return [(x >> k) & 1 for k in range(width-1, -1, -1)]

def triple_to_bits(tri):
    h,r,t = tri
    hid = ent2id[_strip_uri_datatype(h)]
    rid = rel2id[str(r)]
    tid = ent2id[_strip_uri_datatype(t)]
    bits = int_to_bits(hid, B_ent) + int_to_bits(rid, B_rel) + int_to_bits(tid, B_ent)
    return bits

# unique triples만 SSC에 저장
seen=set()
ssc=[]
for h,r,t in all_triples:
    key=(h,r,t)
    if key in seen:
        continue
    seen.add(key)
    ssc.append({"triple":[h,r,t], "bits": triple_to_bits([h,r,t])})

with open(OUT_SSC, "w", encoding="utf-8") as f:
    json.dump(ssc, f, ensure_ascii=False, indent=2)
print("unique triples:", len(ssc))
print("saved:", OUT_SSC)
import os, json, re
from collections import defaultdict
from tqdm import tqdm

KG_PATH    = "/content/webnlg_kg_text2kg_1000/kg_triples_ssc_train_10016.json"
PAIRS_PATH = "/content/webnlg_kg_text2kg_1000/pairs_train_10016.json"
OUT_PATH   = "/content/webnlg_kg_text2kg_1000/semantic_symbol_abstraction_train_10016.json"

assert os.path.exists(KG_PATH), KG_PATH
assert os.path.exists(PAIRS_PATH), PAIRS_PATH

with open(KG_PATH, "r", encoding="utf-8") as f:
    kg_triples = json.load(f)   # each: {"triple":[h,r,t], "bits":[...]}
with open(PAIRS_PATH, "r", encoding="utf-8") as f:
    pairs = json.load(f)        # each: {"idx":..., "text":..., "triples":[...]}

print("KG:", KG_PATH)
print("Pairs:", PAIRS_PATH)
print("KG triples:", len(kg_triples))
print("Pairs:", len(pairs))

# -------------------------
# Normalization (match-friendly)
# -------------------------
def strip_datatype(x: str) -> str:
    x = str(x).strip()
    if "^^<" in x:
        x = x.split("^^<")[0].strip()
    return x

def norm(x: str) -> str:
    x = strip_datatype(x)
    x = x.replace("_", " ").replace('"', " ").strip()
    x = re.sub(r"^<http[^>]+/([^/>]+)>$", r"\1", x)
    x = re.sub(r"\s+", " ", x)
    return x.lower()

# -------------------------
# Build head index
# -------------------------
head_index = defaultdict(list)

# ✅ FIX: tr["h"]가 아니라 tr["triple"][0]을 head로
for tr in kg_triples:
    if "triple" not in tr or not isinstance(tr["triple"], list) or len(tr["triple"]) < 3:
        continue
    h, r, t = tr["triple"][0], tr["triple"][1], tr["triple"][2]
    head_index[norm(h)].append(tr)

all_heads = list(head_index.keys())
print("Unique heads indexed:", len(all_heads))

# -------------------------
# Text2KG-style alignment (Table I)
# - for each sentence s
# - for each triple(h,r,t) in KG
#   if s contains h and s contains t => select
#
# But brute force is huge, so we:
#  1) find which KG heads appear in s (via simple scan)
#  2) only test triples under those heads
# -------------------------
def find_candidate_heads_in_text(s_norm: str, heads: list):
    # 간단 버전: head 문자열이 text에 포함되는지 체크
    # (더 빠르게 하려면 Aho-corasick 같은 걸 쓰지만, 우선은 이걸로 충분)
    cand = []
    for h in heads:
        if h and h in s_norm:
            cand.append(h)
    return cand

results = []
skipped_no_head = 0

# heads가 너무 많으면 포함 체크가 느릴 수 있어서
# head 길이가 짧은 것들(예: 1~2글자)은 오탐 많아 제외하는 옵션
MIN_HEAD_LEN = 3

heads_filtered = [h for h in all_heads if len(h) >= MIN_HEAD_LEN]

for ex in tqdm(pairs, total=len(pairs)):
    idx = ex.get("idx")
    text = ex.get("text", "")
    s_norm = norm(text)

    cand_heads = find_candidate_heads_in_text(s_norm, heads_filtered)
    if not cand_heads:
        skipped_no_head += 1
        results.append({
            "idx": idx,
            "text": text,
            "semantic_symbols": [],   # aligned triples (KG 기준)
        })
        continue

    aligned = []
    for h in cand_heads:
        for tr in head_index[h]:
            hh, rr, tt = tr["triple"]
            # Table I 조건: s.contains(h) AND s.contains(t)
            if norm(tt) in s_norm:
                aligned.append({
                    "triple": tr["triple"],
                    "bits": tr["bits"]
                })

    results.append({
        "idx": idx,
        "text": text,
        "semantic_symbols": aligned
    })

print("Skipped (no head found):", skipped_no_head, "/", len(pairs))

with open(OUT_PATH, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print("Saved:", OUT_PATH)

# quick sanity check
nonempty = sum(1 for x in results if len(x["semantic_symbols"]) > 0)
print("Non-empty semantic symbols:", nonempty, "/", len(results))
print("Example non-empty:")
for x in results:
    if x["semantic_symbols"]:
        print("idx:", x["idx"])
        print("text:", x["text"][:160])
        print("first symbol:", x["semantic_symbols"][0]["triple"])
        break
